{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title for the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ©º Medical Q&A Chatbot using LangChain, Chroma & FLAN-T5\n"
     ]
    }
   ],
   "source": [
    "# Title for the notebook\n",
    "print(\"ðŸ©º Medical Q&A Chatbot using LangChain, Chroma & FLAN-T5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and initialize the system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preprocessing dataset...\n",
      "Dataset loaded.\n",
      "Step 2: Loading documents...\n",
      "Step 3: Splitting text into chunks...\n",
      "Step 4: Generating embeddings using HuggingFace...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec4aeffeac4654b9c0fabf531ded22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc802ebc4ea4cc4a8b9d5937b78d06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b3eaf714a348179f611faf5c7e539c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86228e49dca4515aed8c7f81fa64e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14df8b510a1491cabf3e2dbd4a27e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780495a5975e46d38d23bc46c4611e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26672472b47448cb58e1c5f5c495a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caa1a1f1f4042779072fac26a616b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08952b18f0c4c8bb59faed9281f74e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43950691fe634a878344baa2f06f94e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d2278dcf134854becc42069969205c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Creating Chroma vector store...\n",
      "Step 6: Loading FLAN-T5 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ae5e037a944b4581b3688f13aba5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ee5683976e47a788859a2c4cdc2503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f24b1f1718b4d7cbf61cdc232740b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc8341c83854ea59e19e7429285a2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5786f6158e2b4f3ab2002fee2ce26f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78156730fbc84ebb906979dd0dfb3cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56178f6ea274372b286b9cb9d6e3ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Building QA Chain...\n",
      "âœ… System is ready! You can now ask your medical questions.\n"
     ]
    }
   ],
   "source": [
    "# Load and initialize the system\n",
    "def load_and_initialize_system():\n",
    "    print(\"Step 1: Loading and preprocessing dataset...\")\n",
    "    data = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\", split='train')\n",
    "    df = data.to_pandas()\n",
    "    df = df[:100]  # For demo\n",
    "\n",
    "    print(\"Dataset loaded.\")\n",
    "\n",
    "    # Step 2: Document loader\n",
    "    print(\"Step 2: Loading documents...\")\n",
    "    df_loader = DataFrameLoader(df, page_content_column=\"Answer\")\n",
    "    documents = df_loader.load()\n",
    "\n",
    "    # Step 3: Split documents\n",
    "    print(\"Step 3: Splitting text into chunks...\")\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1250, separator=\"\\n\", chunk_overlap=100)\n",
    "    split_texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Step 4: Generate embeddings\n",
    "    print(\"Step 4: Generating embeddings using HuggingFace...\")\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Step 5: Create Chroma DB\n",
    "    print(\"Step 5: Creating Chroma vector store...\")\n",
    "    chroma_db = Chroma.from_documents(split_texts, embedder, persist_directory=\"chromadb\")\n",
    "\n",
    "    # Step 6: Load seq2seq model\n",
    "    print(\"Step 6: Loading FLAN-T5 model...\")\n",
    "    hf_pipeline = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=\"google/flan-t5-large\",  # Better model for medical Q&A\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "    # Step 7: Setup memory\n",
    "    memory = ConversationBufferWindowMemory(memory_key='chat_history', k=4, return_messages=True)\n",
    "\n",
    "    # Step 8: Create Conversational Retrieval Chain\n",
    "    print(\"Step 8: Building QA Chain...\")\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=chroma_db.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    print(\"âœ… System is ready! You can now ask your medical questions.\")\n",
    "    return qa_chain\n",
    "\n",
    "# Load the system\n",
    "qa_chain = load_and_initialize_system()\n",
    "\n",
    "# Function to ask questions\n",
    "def ask_question(question):\n",
    "    formatted_question = (\n",
    "        f\"Act like a professional doctor. Based on the context and your knowledge, \"\n",
    "        f\"answer clearly:\\n\\nQuestion: {question}\"\n",
    "    )\n",
    "    result = qa_chain.invoke({\"question\": formatted_question})\n",
    "    return result[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preprocessing dataset...\n",
      "Dataset loaded.\n",
      "Step 2: Loading documents...\n",
      "Step 3: Splitting text into chunks...\n",
      "Step 4: Generating embeddings using HuggingFace...\n",
      "Step 5: Creating Chroma vector store...\n",
      "Step 6: Loading FLAN-T5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Building QA Chain...\n",
      "âœ… System is ready! You can now ask your medical questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "/home/sagaryadav/anaconda3/envs/chatbot/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: I have a patient with persistent cough and weight loss. How can I confirm a diagnosis of tuberculosis?\n",
      "Answer: Diagnosis often is made by ruling out other infections that cause similar symptoms.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagaryadav/anaconda3/envs/chatbot/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sagaryadav/anaconda3/envs/chatbot/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the system\n",
    "qa_chain = load_and_initialize_system()\n",
    "\n",
    "# Function to ask questions\n",
    "def ask_question(question):\n",
    "    formatted_question = (\n",
    "        f\"Act like a professional doctor. Based on the context and your knowledge, \"\n",
    "        f\"answer clearly:\\n\\nQuestion: {question}\"\n",
    "    )\n",
    "    result = qa_chain.invoke({\"question\": formatted_question})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# Example usage\n",
    "questions = [\n",
    "    \"I have a patient with persistent cough and weight loss. How can I confirm a diagnosis of tuberculosis?\",\n",
    "    \"My patient presents with acute chest pain. What tests should I order to rule out myocardial infarction?\",\n",
    "    \"I have a patient with persistent fever and lymphadenopathy. How do I confirm lymphoma?\"\n",
    "]\n",
    "\n",
    "# Ask a question\n",
    "for q in questions:\n",
    "    answer = ask_question(q)\n",
    "    print(f\"Question: {q}\\nAnswer: {answer}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
